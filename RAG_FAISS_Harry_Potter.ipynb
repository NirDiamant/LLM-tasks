{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOuy9XMw26g0CxzYCB1XC5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NirDiamant/LLM-tasks/blob/main/RAG_FAISS_Harry_Potter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "b-WmQzFRgjme"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqwxMyWGggao"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install -qU langchain accelerate bitsandbytes transformers sentence-transformers faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "CkW1Xv3-gojh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries and modules\n",
        "import os\n",
        "from glob import glob\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "r71VXrHBgoAA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Initialize Model and Tokenizer with BitsAndBytes Configuration"
      ],
      "metadata": {
        "id": "Oa0i9H1Fgtw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure BitsAndBytes for efficient model loading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# Load the model with the above configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    quantization_config=bnb_config,\n",
        "    do_sample=False,\n",
        "\n",
        ")\n",
        "\n",
        "# Initialize tokenizer and set padding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "Ywl7JOaYgwID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Text Generation Pipeline"
      ],
      "metadata": {
        "id": "MD0Qp4yegyxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the text generation pipeline with specific parameters\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0,\n",
        "    task=\"text-generation\",\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=2000,\n",
        "\n",
        ")\n",
        "\n",
        "# Create a HuggingFacePipeline instance for text generation\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "4EISJk0mg057"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Prompt Template"
      ],
      "metadata": {
        "id": "FyWrmjiTg2oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template for generating text\n",
        "prompt_template = \"\"\"\n",
        "Instruction: prompt=f\"Answer the following question based only on the provided context:{context}\n",
        "and then print \"reference:\" and show me from which part of the context your retrieved this answer.\n",
        "if the answer doesn't appear in the context, answer: \\\"No answer available\\\"\"\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the PromptTemplate with the defined template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")"
      ],
      "metadata": {
        "id": "pj69TIbng4HP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create LLM Chain"
      ],
      "metadata": {
        "id": "T6qZ8c2-g6qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "1-D3ItASg75v"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load and Process PDF Documents"
      ],
      "metadata": {
        "id": "D9TdHt6jhJXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "bXiV1D1r6jh3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pypdf"
      ],
      "metadata": {
        "id": "Czg89PGC2W-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path =\"/content/Harry Potter - Book 1 - The Sorcerers Stone.pdf\""
      ],
      "metadata": {
        "id": "A0ID1PcyCsvE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(path)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "tm6FsEkTG0ah"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a deep copy of the pages list to retain original objects intact\n",
        "import copy\n",
        "cleaned_pages = copy.deepcopy(pages)\n",
        "\n",
        "# Clean the page_content of each page in the cleaned_pages list\n",
        "for page in cleaned_pages:\n",
        "    page.page_content = page.page_content.replace('\\t', ' ')  # Replace tab characters with spaces\n",
        "\n",
        "# Now, cleaned_pages contains the original objects with cleaned page_content\n"
      ],
      "metadata": {
        "id": "pi7WA_RmbCFi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Documents with FAISS"
      ],
      "metadata": {
        "id": "s1BbWgwrhQEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index the processed documents with FAISS for efficient retrieval\n",
        "db = FAISS.from_documents(\n",
        "    cleaned_pages,\n",
        "    HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        ")\n",
        "\n",
        "# Convert the FAISS index into a retriever\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "ZC_QkHqkhQos"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the retriever db"
      ],
      "metadata": {
        "id": "WjkCPUkorMYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
        "def load_object(filename):\n",
        "    with open(filename, 'rb') as inp:  # Open the file in binary read mode\n",
        "        obj = pickle.load(inp)\n",
        "    return obj"
      ],
      "metadata": {
        "id": "ppLppSZ-d1pv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_object(retriever, 'hp_retriever.pkl')"
      ],
      "metadata": {
        "id": "8V_TLk2Sd2zl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rXydlmZYgUIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the source path of your file (the one you want to copy)\n",
        "source_path = '/content/hp_retriever.pkl'\n",
        "\n",
        "# Define the destination path in your Google Drive\n",
        "destination_path = '/content/drive/My Drive/rag/retriever_file_hp'  # Update this path"
      ],
      "metadata": {
        "id": "zWIvyfjQgyQS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying the file\n",
        "import shutil\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"File copied to Google Drive successfully: {destination_path}\")"
      ],
      "metadata": {
        "id": "KkNNEhk6lylN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_retriever= load_object(destination_path)"
      ],
      "metadata": {
        "id": "0NGQyiv5hOV1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Chain and Retrieve Response"
      ],
      "metadata": {
        "id": "-v6KIx9ihjAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the RAG (Retrieval-Augmented Generation) chain\n",
        "rag_chain = (\n",
        "    {\"context\": loaded_retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "# Invoke the chain with a specific question\n",
        "response = rag_chain.invoke(\"who is sirius black?\")\n",
        "\n",
        "# Print the question and the generated response\n",
        "print(\"Question:\", response[\"question\"])\n",
        "print(response[\"text\"])\n"
      ],
      "metadata": {
        "id": "eCT9Y_P2hkjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}