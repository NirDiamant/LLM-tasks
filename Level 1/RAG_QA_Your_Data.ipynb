{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KQAp40FweQr"
      },
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d99xgkiDwTce"
      },
      "outputs": [],
      "source": [
        "pip install transformers sentence-transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gmXkE-wwlMS"
      },
      "source": [
        "## Load the Wikipedia dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_3cwz61wW5S",
        "outputId": "87995a01-83b8-43e6-ac49-95d94075aa89"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split='train[:1%]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwnWIWdfOkia"
      },
      "source": [
        "## Preprocess the dataset to extract sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IgkytxZNOh8h"
      },
      "outputs": [],
      "source": [
        "def extract_sentences(example):\n",
        "    return {\"sentence\": example[\"text\"].split(\".\")}\n",
        "\n",
        "sentences_dataset = dataset.map(extract_sentences, remove_columns=[\"text\", \"title\"])  # remove_columns=[\"text\", \"title\"]\n",
        "\n",
        "# Flatten the list of sentences\n",
        "flattened_sentences = [sentence for sublist in sentences_dataset[\"sentence\"] for sentence in sublist if sentence.strip() != \"\"]\n",
        "\n",
        "# choose any subset for simplicity\n",
        "facts_database = flattened_sentences[:500000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3FgZF0Q5JL"
      },
      "source": [
        "## Load sentence transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpRXe9owwz1J"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3tz5Dy72q9"
      },
      "source": [
        "## Encode the facts into embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kmrd1FtQw6Y"
      },
      "outputs": [],
      "source": [
        "fact_embeddings = model.encode(facts_database, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m99kb1whwzCP"
      },
      "source": [
        "# Semantic Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_D8a1X5KwD"
      },
      "source": [
        "## creating an index using FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDZx9ZIr1cYq"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO-pDAkqRZa7"
      },
      "source": [
        "## Define a function that creates a FAISS index from the database embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BhMhAAic1Q6C"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def create_train_faiss_index(fact_embeddings_np, nprobe_ratio=1/4, num_centroids_ratio=40):\n",
        "    \"\"\"\n",
        "    Initializes, trains, and returns a FAISS IndexIVFFlat index for the given embeddings.\n",
        "\n",
        "    Args:\n",
        "        fact_embeddings_np (numpy.ndarray): The embeddings array.\n",
        "        nprobe_ratio (float): The ratio of the number of centroids to search in the IVF index, affects search accuracy vs speed.\n",
        "        num_centroids_ratio (int): The ratio to determine the number of centroids based on dataset size.\n",
        "\n",
        "    Returns:\n",
        "        faiss.IndexIVFFlat: The trained FAISS index.\n",
        "    \"\"\"\n",
        "    embed_length = fact_embeddings_np.shape[1]\n",
        "\n",
        "    # Determine the number of centroids based on the heuristic and dataset size\n",
        "    num_centroids = min(1024, len(fact_embeddings_np) // num_centroids_ratio)\n",
        "\n",
        "    # Initialize the quantizer using L2 distance\n",
        "    quantizer = faiss.IndexFlatL2(embed_length)\n",
        "\n",
        "    # Create the IndexIVFFlat index\n",
        "    index = faiss.IndexIVFFlat(quantizer, embed_length, num_centroids, faiss.METRIC_L2)\n",
        "\n",
        "    # Train the index with embeddings\n",
        "    assert not index.is_trained, \"Index should not be trained yet.\"\n",
        "    index.train(fact_embeddings_np)\n",
        "    assert index.is_trained, \"Index training failed.\"\n",
        "\n",
        "    # Add embeddings to the index\n",
        "    index.add(fact_embeddings_np)\n",
        "\n",
        "    # Adjust nprobe for balancing search accuracy vs speed\n",
        "    index.nprobe = max(1, min(64, num_centroids // int(1/nprobe_ratio)))\n",
        "\n",
        "    return index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-eskpnWRlAx"
      },
      "source": [
        "## Use the function to create the FAISS index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLKUwKeMRqrz"
      },
      "outputs": [],
      "source": [
        "index = create_train_faiss_index(fact_embeddings_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e44s4pNb3j61"
      },
      "source": [
        "## Search the Index with a Query\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKGMnoaP3jHG"
      },
      "outputs": [],
      "source": [
        "query_text = \"What is the biggest animal?\"\n",
        "query_embedding = model.encode([query_text])\n",
        "\n",
        "# Perform search\n",
        "top_k = 10\n",
        "scores, index_vals = index.search(query_embedding, top_k)\n",
        "\n",
        "# Display results\n",
        "for rank, (score, idx) in enumerate(zip(scores[0], index_vals[0]), start=1):\n",
        "    print(f\"Rank {rank}: {facts_database[idx]} (Score: {score})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toBls3qlGBRU"
      },
      "source": [
        "### As you can notice, the ranking doesn't match the real order of answers you would expect, since FAISS looks for the sematic similarity between the query and the sentences in the database. We will use a function to re-rank the results, based on a QA model to solve this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT3751SUWIkv"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a QA model\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsD7J_FIEGqG"
      },
      "outputs": [],
      "source": [
        "def rerank_candidates(question, candidates):\n",
        "    reranked_scores = []\n",
        "    for candidate in candidates:\n",
        "        # Use the QA model to evaluate the candidate\n",
        "        result = qa_pipeline(question=question, context=candidate)\n",
        "        score = result[\"score\"]  # The model's confidence in its answer\n",
        "        reranked_scores.append((score, candidate))\n",
        "\n",
        "    # Sort based on score\n",
        "    reranked_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "    return reranked_scores\n",
        "\n",
        "# Assuming query_embedding is correctly prepared and represents one query\n",
        "top_k = 20\n",
        "scores, index_vals = index.search(query_embedding, top_k)\n",
        "\n",
        "# Correctly retrieve candidates based on indices\n",
        "candidates = [facts_database[idx] for idx in index_vals[0]]  # Access the first row of index_vals\n",
        "\n",
        "reranked = rerank_candidates(query_text, candidates)\n",
        "\n",
        "# Display top re-ranked answer\n",
        "print(f\"Top Answer: {reranked[0][1]} (Score: {reranked[0][0]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRA2_2MAGtRA"
      },
      "source": [
        "## Let's wrap it up with a function that will receive a query question and will return the final re-ranked answer:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iLrhKb9THOrx"
      },
      "outputs": [],
      "source": [
        "def answer_query(query, facts_database, index, model, top_k=5):\n",
        "    \"\"\"\n",
        "    Receives a query, searches in the index, re-ranks with the QA model, and returns the final answer.\n",
        "\n",
        "    Args:\n",
        "        query (str): The question to answer.\n",
        "        facts_database (List[str]): The database of facts to search from.\n",
        "        index (faiss.Index): The FAISS index for retrieval.\n",
        "        model (SentenceTransformer): The sentence transformer model for encoding.\n",
        "        top_k (int): Number of top candidates to retrieve and re-rank.\n",
        "\n",
        "    Returns:\n",
        "        str: The final answer to the query.\n",
        "    \"\"\"\n",
        "    # Encode the query\n",
        "    query_embedding = model.encode([query])\n",
        "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
        "\n",
        "    # Search the index\n",
        "    _, index_vals = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve candidate sentences\n",
        "    candidates = [facts_database[idx] for idx in index_vals[0]]\n",
        "\n",
        "    # Re-rank candidates using the QA model\n",
        "    reranked_scores = []\n",
        "    for candidate in candidates:\n",
        "        result = qa_pipeline(question=query, context=candidate)\n",
        "        score = result[\"score\"]\n",
        "        reranked_scores.append((score, candidate, result[\"answer\"]))\n",
        "\n",
        "    reranked_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # Return the top answer\n",
        "    top_answer = reranked_scores[0][2]  # Access the answer from the top tuple\n",
        "    return top_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdAAcZvqHVwy"
      },
      "outputs": [],
      "source": [
        "query_text = \"What is the biggest animal?\"\n",
        "final_answer = answer_query(query_text, facts_database, index, model, top_k=20)\n",
        "print(f\"Answer: {final_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G6D4jI4YJkp"
      },
      "source": [
        "The results might still be incorrect.\n",
        "Possible reasons for that:\n",
        "\n",
        "\n",
        "*   The answer doesn't appear in the database\n",
        "*   The structure of the database isn't informative enough (only sentences).\n",
        "*   Sentence Encoder not suitable for the problem\n",
        "*   Inappropriate Semantic Retrieval params\n",
        "*   QA model not suitable for reranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxX6hNLrKXmV"
      },
      "source": [
        "## Now let's try split the Wikipedia database into paragraphs instead of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VznIplARLXwk"
      },
      "outputs": [],
      "source": [
        "# Function to split text into paragraphs\n",
        "def extract_paragraphs(batch):\n",
        "    # Process a batch of texts to extract paragraphs without flattening across the entire batch\n",
        "    paragraphs_batch = [[para for para in text.split('\\n\\n') if para.strip() != ''] for text in batch['text']]\n",
        "    return {'paragraph': paragraphs_batch}\n",
        "\n",
        "# Apply the function in a batched manner\n",
        "paragraph_dataset = dataset.map(extract_paragraphs, batched=True, remove_columns=['text', 'title'])\n",
        "flattened_paragraphs = [paragraph for sublist in paragraph_dataset['paragraph'] for paragraph in sublist if paragraph.strip() != \"\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1OpYbVtZjKl"
      },
      "source": [
        "## Running the whole pipline straight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVB6itCjLXEp"
      },
      "outputs": [],
      "source": [
        "paragraphs_database = flattened_paragraphs[:500000]\n",
        "paragraphs_embeddings = model.encode(paragraphs_database, convert_to_tensor=True)\n",
        "paragraphs_embeddings_np = paragraphs_embeddings.cpu().detach().numpy()\n",
        "paragraph_index = create_train_faiss_index(paragraphs_embeddings_np, nprobe_ratio=1/4, num_centroids_ratio=40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqmoIYI6Zlf3"
      },
      "source": [
        "## Check the new result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWe35E2fVu9U",
        "outputId": "5d066784-3d48-4a1e-a1b1-79f044e58fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: President of Congress\n"
          ]
        }
      ],
      "source": [
        "query_text = \"who is the president of the usa?\"\n",
        "final_answer = answer_query(query_text, paragraphs_database, paragraph_index, model, top_k=20)\n",
        "print(f\"Answer: {final_answer}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
